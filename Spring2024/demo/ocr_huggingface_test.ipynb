{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P4Wt6-UqVeAk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.1.0 Requires-Python >=3.9; 0.1.0rc1 Requires-Python >=3.8; 0.1.0rc2 Requires-Python >=3.8; 0.1.0rc3 Requires-Python >=3.9; 0.2.0 Requires-Python >=3.9; 0.2.1 Requires-Python >=3.9; 0.2.2 Requires-Python >=3.9; 0.3.0 Requires-Python >=3.9; 0.3.1 Requires-Python >=3.9; 0.3.2 Requires-Python >=3.9; 0.4.0 Requires-Python >=3.9; 0.4.1 Requires-Python >=3.9; 0.5.0 Requires-Python >=3.9; 0.5.1 Requires-Python >=3.9; 0.5.2 Requires-Python >=3.9\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement google-generativeai (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-generativeai\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-cloud-documentai google-cloud-storage google-cloud-translate google-generativeai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jsee7lWVc5O",
        "outputId": "706c9a69-5e9c-466e-879b-1d061f4481b4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import documentai_v1 as documentai\n",
        "from google.cloud.documentai_v1.types import RawDocument\n",
        "import zipfile\n",
        "import os\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import textwrap\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown\n",
        "import random\n",
        "import re\n",
        "from time import sleep\n",
        "\n",
        "# —————————————\n",
        "#     SETUP\n",
        "# —————————————\n",
        "\n",
        "# CREDENTIALS FILE PATH\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/gtrammell/Desktop/herbaria-ai-3c860bcb0f44.json\"\n",
        "\n",
        "# GEMINI API KEY\n",
        "genai.configure(api_key='')\n",
        "\n",
        "# GEMINI MODEL DECLARATION\n",
        "model = genai.GenerativeModel('gemini-1.0-pro')\n",
        "\n",
        "# DOCUMENT AI DETAILS\n",
        "project_id = \"herbaria-ai\"\n",
        "location = \"us\"\n",
        "processor_id = \"de954414712822b3\"\n",
        "\n",
        "# helper function for processing gemini responses (which are in markdown)\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "# few-shot samples\n",
        "shots = \\\n",
        "{\n",
        "    \"Chinese National Herbarium (PE) Plants of Xizang CHINA, Xizang, Lhoka City, Lhozhag County, Lhakang Town, Kharchhu Gompa vicinity 西藏自治区山南市洛扎县拉康镇卡久寺附近 28°5'37.15N, 91°7'24.74″E; 3934 m Herbs. Slopes near roadsides. PE-Xizang Expedition #PE6657 14 September 2017 M4 5 6 7 8 NCIL 中国数字植物标本馆 N° 2604988 西藏 TIBET 中国科学院 植物研究所 标本馆 PE CHINESE NATIONAL HERBARIUM (PE) 02334122 #PE6657 ASTERACEAE 菊科 Anaphalis contorta (D. Don) Hook. f. 鉴定人:张国进 Guo-Jin ZHANG 旋叶香青 17 May 2018\"\n",
        "    :{\"Collector\":\"Guo-Jin, Zhang\",\n",
        "      \"Location\":\"Xizang Autonomous Region, Shannan City, Lhozhag County, Lhakang Town, Kharchhu Gompa vincinity, Slopes near roadsides\",\n",
        "      \"Taxon\":\"Asteraceae; Anaphalis contorta (D. Don) Hook. f.\",\n",
        "      \"Date\":\"14 September 2017\",\n",
        "      \"Confidence\":\".94\"\n",
        "    },\n",
        "\n",
        "    \"PE-Xizang Expedition #PE6673 9 NSIT Chinese National Herbarium (PE) Plants of Xizang CHINA, Xizang, Lhoka City, Lhozhag County, Lhakang Town, Kharchhu Gompa vicinity 28°5&#39;37.15&quot;N, 91°7&#39;24.74&quot;E; 3934 m Herbs. Slopes near roadsides. PE-Xizang Expedition #PE6673 9 NSIT Chinese National Herbarium (PE) Plants of Xizang CHINA, Xizang, Lhoka City, Lhozhag County, Lhakang Town, Kharchhu Gompa vicinity 28°5&#39;37.15&quot;N, 91°7&#39;24.74&quot;E; 3934 m Herbs. Slopes near roadsides. PE-Xizang Expedition #PE6673 9 NSIT Chinese National Herbarium (PE) Plants of Xizang Spiral Leaf Green 17 May 2018\"\n",
        "    :{\"Collector\":\"PE-Xizang Expedition #PE6673\",\n",
        "      \"Location\":\"Xizang Autonomous Region, Lhoka City, Lhozhag County, Lhakang Town, Kharchhu Gompa vicinity, Slopes near roadsides\",\n",
        "      \"Taxon\":\"Spiral Leaf Green\",\n",
        "      \"Date\":\"17 May 2018\",\n",
        "      \"Confidence\":\".76\"\n",
        "    },\n",
        "\n",
        "    \"Honey Plants Research Institute of the Chinese Academy of Agricultural Sciences Collection No.: 13687. May 7, 1993 Habitat Roadside Altitude: 1600 * Characters Shrub No. Herbarium of the Institute of Botany, Chinese Academy of Sciences Collector 3687 Scientific Name Height: m (cm) Diameter at breast height m (cm) Flower: White Fruit: Notes Blooming period: from January to July Honey: Scientific Name: Rosa Sericea Lindl. Appendix: Collector: cm 1 2 3 4 25 CHINESE NATIONAL HERBARUM ( 01833954 No 1479566 * Herbarium of the Institute of Botany, Chinese Academy of Sciences Sichuan SZECHUAN DET. Rosa sercea Lindl. var. Various Zhi 2009-02-16\"\n",
        "    :{\"Collector\":\"UNKNOWN\",\n",
        "      \"Location\":\"Sichuan, China, Roadside, Altitude: 1600\",\n",
        "      \"Taxon\":\"Rosa sericea\",\n",
        "      \"Date\":\"7 May 1993\",\n",
        "      \"Confidence\":\".45\"\n",
        "    },\n",
        "}\n",
        "\n",
        "# ————————————\n",
        "#     FUNC\n",
        "# ————————————\n",
        "\n",
        "# few-shot randomizer\n",
        "def get_random_pairs_list(input_dict, num_pairs=3):\n",
        "    if len(input_dict) < num_pairs:\n",
        "        return \"Not enough elements in the dictionary to select the requested number of pairs\"\n",
        "    keys = random.sample(list(input_dict.keys()), num_pairs)\n",
        "    return [(key, input_dict[key]) for key in keys]\n",
        "\n",
        "# main gemini processor\n",
        "def generate_metadata(results_df, shots):\n",
        "    responses = []\n",
        "    for input_text in results_df[\"extracted_text\"]:\n",
        "\n",
        "        # FEW-SHOT RANDOMIZER\n",
        "        random_pairs = get_random_pairs_list(shots)\n",
        "\n",
        "        # PROMPT FORMATTING\n",
        "        prompt = \\\n",
        "        \"\"\"\n",
        "        Your goal is to translate (if necessary) and then extract four items from a\n",
        "        string of text: the name of the specimen collector, the location, the taxon\n",
        "        and/or any identifying information about the specimen, and the earliest date.\n",
        "        Your response should contain only JSON. Use the best information available\n",
        "        or insert 'UNKNOWN' if there is none. Provide a rough estimate of confidence\n",
        "        in your output ranging from 0-1 inside your JSON output.\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        Input 1:\n",
        "        {shot1_input}\n",
        "        Output 1:\n",
        "        {{\"Collector\":\"{shot1_output_collector}\",\"Location\":\"{shot1_output_location}\",\"Taxon\":\"{shot1_output_taxon}\",\"Date\":\"{shot1_output_date}\",\"Confidence\":\"{shot1_confidence}\"}}\n",
        "\n",
        "        Input 2:\n",
        "        {shot2_input}\n",
        "        Output 2:\n",
        "        {{\"Collector\":\"{shot2_output_collector}\",\"Location\":\"{shot2_output_location}\",\"Taxon\":\"{shot2_output_taxon}\",\"Date\":\"{shot2_output_date},\"Confidence\":\"{shot2_confidence}\"}}\n",
        "\n",
        "        Input 3:\n",
        "        {shot3_input}\n",
        "        Output 3:\n",
        "        {{\"Collector\":\"{shot3_output_collector}\",\"Location\":\"{shot3_output_location}\",\"Taxon\":\"{shot3_output_taxon}\",\"Date\":\"{shot3_output_date},\"Confidence\":\"{shot3_confidence}\"}}\n",
        "\n",
        "        Your attempt:\n",
        "        Input:\n",
        "        {input_text}\n",
        "        Output:\n",
        "\n",
        "        \"\"\".format(\n",
        "        shot1_input = random_pairs[0][0],\n",
        "        shot1_output_collector = random_pairs[0][1]['Collector'],\n",
        "        shot1_output_location = random_pairs[0][1]['Location'],\n",
        "        shot1_output_taxon = random_pairs[0][1]['Taxon'],\n",
        "        shot1_output_date = random_pairs[0][1]['Date'],\n",
        "        shot1_confidence = random_pairs[0][1]['Confidence'],\n",
        "\n",
        "        shot2_input = random_pairs[1][0],\n",
        "        shot2_output_collector = random_pairs[1][1]['Collector'],\n",
        "        shot2_output_location = random_pairs[1][1]['Location'],\n",
        "        shot2_output_taxon = random_pairs[1][1]['Taxon'],\n",
        "        shot2_output_date = random_pairs[1][1]['Date'],\n",
        "        shot2_confidence = random_pairs[1][1]['Confidence'],\n",
        "\n",
        "        shot3_input = random_pairs[2][0],\n",
        "        shot3_output_collector = random_pairs[2][1]['Collector'],\n",
        "        shot3_output_location = random_pairs[2][1]['Location'],\n",
        "        shot3_output_taxon = random_pairs[2][1]['Taxon'],\n",
        "        shot3_output_date = random_pairs[2][1]['Date'],\n",
        "        shot3_confidence = random_pairs[2][1]['Confidence'],\n",
        "\n",
        "        input_text = input_text\n",
        "        )\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        responses.append(response)\n",
        "\n",
        "    return responses\n",
        "\n",
        "# gemini response handler\n",
        "def process_responses(responses):\n",
        "    text_responses = []\n",
        "    for response in responses:\n",
        "        extracted_text = to_markdown(response.text).data\n",
        "        text_responses.append(extracted_text.strip().replace('>', '')[1:])\n",
        "\n",
        "    json_responses = []\n",
        "    for text in text_responses:\n",
        "        try:\n",
        "            json_response = json.loads(re.search(r'{.*}', text, re.DOTALL).group())\n",
        "            json_responses.append(json_response)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"Failed on input\", text_responses.index(text), \"| Reason:\", e)\n",
        "            continue\n",
        "\n",
        "    return json_responses\n",
        "\n",
        "# main document AI processor\n",
        "def batch_process_documents(file_path: str, file_mime_type: str) -> tuple:\n",
        "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
        "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
        "\n",
        "    with open(file_path, \"rb\") as file_stream:\n",
        "        raw_document = RawDocument(content=file_stream.read(), mime_type=file_mime_type)\n",
        "\n",
        "    name = client.processor_path(project_id, location, processor_id)\n",
        "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
        "    result = client.process_document(request=request)\n",
        "\n",
        "    extracted_text = result.document.text.replace('\\n', ' ')\n",
        "    return extracted_text\n",
        "\n",
        "# file upload\n",
        "def unzip_and_find_jpgs(file_path):\n",
        "    extract_path = \"extracted_files\"\n",
        "    if os.path.exists(extract_path):\n",
        "        # clear dir\n",
        "        for root, dirs, files in os.walk(extract_path, topdown=False):\n",
        "            for name in files:\n",
        "                os.remove(os.path.join(root, name))\n",
        "            for name in dirs:\n",
        "                os.rmdir(os.path.join(root, name))\n",
        "        os.rmdir(extract_path)\n",
        "\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    jpg_files = []\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "        for root, dirs, files in os.walk(extract_path):\n",
        "            if '__MACOSX' in root:\n",
        "                continue\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.jpg'):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    jpg_files.append(full_path)\n",
        "    return jpg_files\n",
        "\n",
        "# ————————————\n",
        "#     MAIN\n",
        "# ————————————\n",
        "\n",
        "def process_images(uploaded_file):\n",
        "    # make new dataframe each time this function is called\n",
        "    results_df = pd.DataFrame(columns=[\"filename\", \"collector\", \"location\", \"taxon\", \"date\", \"confidence\", \"extracted_text\"]) \n",
        "\n",
        "    # easy gradio filename storage\n",
        "    file_path = uploaded_file.name\n",
        "\n",
        "    try:\n",
        "        image_files = unzip_and_find_jpgs(file_path)\n",
        "\n",
        "        if not image_files:\n",
        "            return \"No JPG files found in the zip.\"\n",
        "        \n",
        "        print(image_files)\n",
        "\n",
        "        for file_path in image_files:\n",
        "            # DOCUMENT AI PROCESSING IS HERE\n",
        "            extracted_text = batch_process_documents(file_path, \"image/jpeg\")\n",
        "            new_row = pd.DataFrame([{\n",
        "                \"filename\": os.path.basename(file_path),\n",
        "                \"extracted_text\": extracted_text\n",
        "            }])\n",
        "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
        "\n",
        "        # GEMINI PROCESSING IS HERE\n",
        "        responses = generate_metadata(results_df, shots)\n",
        "        processed_data = process_responses(responses)\n",
        "\n",
        "        # append extracted metadata\n",
        "        for idx, processed in enumerate(processed_data):\n",
        "            results_df.at[idx, \"collector\"] = processed.get(\"Collector\", \"\")\n",
        "            results_df.at[idx, \"location\"] = processed.get(\"Location\", \"\")\n",
        "            results_df.at[idx, \"taxon\"] = processed.get(\"Taxon\", \"\")\n",
        "            results_df.at[idx, \"date\"] = processed.get(\"Date\", \"\")\n",
        "            results_df.at[idx, \"confidence\"] = processed.get(\"Confidence\", \"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)} on file {file_path}\"\n",
        "\n",
        "    html_output = results_df.to_html()\n",
        "\n",
        "    # CSV saving (with temp file)\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
        "    results_df.to_csv(temp_file.name, index=False)\n",
        "\n",
        "    temp_file.close()\n",
        "\n",
        "    # return HTML and output CSV path\n",
        "    return html_output, temp_file.name\n",
        "\n",
        "# ———————————\n",
        "#     UI\n",
        "# ———————————\n",
        "\n",
        "with gr.Blocks() as interface:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"# Herbaria Batch Metadata Extraction\")\n",
        "        gr.Markdown(\"Upload a ZIP file containing JPEG/JPG images, and the system will translate and extract the text from each image.\")\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload ZIP File\")\n",
        "    with gr.Row():\n",
        "        html_output = gr.HTML(label=\"Extracted Text From Your Herbaria Images\")\n",
        "    with gr.Row():\n",
        "        file_output = gr.File(label=\"Download this file to receive the extracted labels from the images.\")\n",
        "\n",
        "    file_input.change(process_images, inputs=file_input, outputs=[html_output, file_output])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
